{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is yes, because K-means is to put each data points into different cluster based on the data point's distance to the cluster center. For Gaussian Mixture it use probability for each data point, so if the Gaussian Mixture have large variance, points in Gasussian Mixture still might be assigned to the same cluster which is same in K-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a): (1)A and C blocked in the condition of B and D is true, because we can see from graph that is head-to-tail both, the path from A to C meet at B and D, and B,D is in set B,D\n",
    "     (2)B and D blocked in the condition of A and C is false, because we can see from graph that is tail-to tail for path B->D meet at A, and A is in set A. for set C is follow the head-to-head, and C shoud not in the set of C\n",
    "     \n",
    "(b): (1)A and C blocked in the condition of B and D is false, because we can see from graph that is head-to-head both, so the node B and D should not in the set B and D\n",
    "     (2)B and D blocked in the condition of A and C is true, because we can see from graph that is tail-to tail both, so the node A and C is in the set A and C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array([[4.6,2.9],[4.7,3.2],[4.9,3.1],[5.0,3.0],[5.1,3.8],[5.5,4.2],[5.9,3.2],[6.0,3.0],[6.2,2.8],[6.7,3.1]])\n",
    "red = np.array([6.2,3.2])\n",
    "blue = np.array([6.5,3.0])\n",
    "green = np.array([6.6,3.7])\n",
    "centroid = np.array([[6.2,3.2],[6.5,3.0],[6.6,3.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to calculate the Euclidean Distance follow the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(p1,p2):\n",
    "    return np.sqrt(np.sum((p1-p2)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to check if the centroid of cluster is converage, and i use and error like 0.00001 to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converage(centroids1,centroids2,error):\n",
    "    return (centroids1 - centroids2).any() < error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans algorithm:\n",
    "      1. First i need to have dataset and inital cluster center, in this question, we already have the center, so i don't need to random create it\n",
    "      2. while the cluster is not converage, for each data point i will calculate the euclidean distance between current data point and each cluster center, choose the minimal one, and update the label of this data point\n",
    "      3. once the cluster is converaged, print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(data,centroid):\n",
    "    iterations = 0\n",
    "    centroids = centroid\n",
    "    is_converaged = False\n",
    "    distance_in_new_centroids = np.zeros(data.shape[0])\n",
    "    labels = np.zeros(data.shape[0])\n",
    "    \n",
    "    \n",
    "    while not is_converaged:\n",
    "        iterations += 1 \n",
    "        previous_centroids = np.copy(centroids)\n",
    "        for i in range(data.shape[0]):\n",
    "            min_distance = np.inf\n",
    "            min_index = -1\n",
    "            for j in range(len(centroid)):\n",
    "                distance = euclidean_distance(data[i],centroids[j])\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    min_index = j\n",
    "                    labels[i] = j\n",
    "            label = labels[i]\n",
    "            distance_in_new_centroids = euclidean_distance(data[i],centroids[np.int(label)])**2\n",
    "            \n",
    "        for z in range(len(centroid)):\n",
    "            centroids[z] = np.mean(data[labels == z],axis = 0)\n",
    "        is_converaged = converage(previous_centroids,centroids,0.00001)\n",
    "        print(\"For iteration %d\" % iterations)\n",
    "        print(\"Centroids is\",centroids)\n",
    "        print(\"cluster scatter\",np.sum(distance_in_new_centroids))\n",
    "        \n",
    "    return centroids,labels,iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 1\n",
      "Centroids is [[5.17142857 3.17142857]\n",
      " [6.45       2.95      ]\n",
      " [5.5        4.2       ]]\n",
      "cluster scatter 0.050000000000000086\n",
      "For iteration 2\n",
      "Centroids is [[4.8   3.05 ]\n",
      " [6.2   3.025]\n",
      " [5.3   4.   ]]\n",
      "cluster scatter 0.08499999999999998\n",
      "For iteration 3\n",
      "Centroids is [[4.8   3.05 ]\n",
      " [6.2   3.025]\n",
      " [5.3   4.   ]]\n",
      "cluster scatter 0.2556250000000001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[4.8  , 3.05 ],\n",
       "        [6.2  , 3.025],\n",
       "        [5.3  , 4.   ]]), array([0., 0., 0., 0., 2., 2., 1., 1., 1., 1.]), 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_means(dataset,centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The center of red cluster after one interation is [5.17142857,3.17142857]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The center of green cluster after two iteration is [5.3,4.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)\n",
    "The center of blue cluster when clustering converages is [6.2,3.025]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from my algorithm that the clusters need 2 iterations to converage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from the txt file, and store in numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('C:\\\\Users\\\\WesleyZhou\\\\Desktop\\\\CS559\\\\Assignment4\\\\multigauss.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the dataset into np.matrix type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.matrix(data,copy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)\n",
    "Define the expection method, which is the a question, and follow the lecture pdf, we can see that expection is to calculate each model's gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expection(X,k,pi,mu,cov):\n",
    "    #number of data points\n",
    "    n = X.shape[0]\n",
    "    # defin the gamma matrix which rows represent number of data points, columns represent gamma\n",
    "    gamma = np.mat(np.zeros((n,k)))\n",
    "    # First i need to calculate the probility of each data points in each models\n",
    "    prob = np.zeros((n,k))\n",
    "    for i in range(k):\n",
    "        min_eig = np.min(np.real(np.linalg.eigvals(cov[i])))\n",
    "        if min_eig < 0:\n",
    "            cov[i] -= 10*min_eig * np.eye(*cov[i].shape)\n",
    "        prob[:,i] = multivariate_normal.pdf(X,mu[i],cov[i])\n",
    "    prob = np.mat(prob)\n",
    "    \n",
    "    # And then i need to calculate each data points' gamma and return the gamma\n",
    "    for j in range(k):\n",
    "        gamma[:,j] = pi[j] * prob[:,j]\n",
    "    for z in range(n):\n",
    "        gamma[z,:] /= np.sum(gamma[z,:])\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b)\n",
    "Define the Maximization of Means,return new maximization of mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaximizeMean(X,k,w):\n",
    "    # define the new mu\n",
    "    mu = np.zeros((k, X.shape[1]))\n",
    "    # update each model\n",
    "    for i in range(k):\n",
    "        #calculate the new nk\n",
    "        nk = np.sum(w[:,i])\n",
    "        #for each features calculate the mean value, and store in the mu array\n",
    "        for j in range(X.shape[1]):\n",
    "            mu[i,j] = np.sum(np.multiply(w[:,i],X[:,j]))/nk\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)\n",
    "Define the Maximization of Covariances, return new covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaximizeCovariance(X,k,w,mu):\n",
    "    #Define the new cov array\n",
    "    cov = []\n",
    "    #update each models\n",
    "    for i in range(k):\n",
    "        # calculate the nk\n",
    "        nk = np.sum(w[:,i])\n",
    "        # define the cov matrix for model i\n",
    "        cov_new = np.mat(np.zeros((X.shape[1],X.shape[1])))\n",
    "        #calculate the calculate the cov matrix and store in cov array\n",
    "        for j in range(X.shape[0]):\n",
    "            cov_new += w[j,i] * (X[j] - mu[i]).T * (X[j] - mu[i]) /nk\n",
    "        cov.append(cov_new)\n",
    "    cov = np.array(cov)\n",
    "    return cov\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d)\n",
    "Define the Maximization of Mixture Weightsï¼Œ return new weight for each gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaximizeMixtures(k,w):\n",
    "    #Define the new pi\n",
    "    pi =  np.zeros(k)\n",
    "    #update for each model\n",
    "    for i in range(k):\n",
    "        # Follow the equation of lecture pdf,we can calculate the new pi for each model, and store in pi array\n",
    "        nk = np.sum(w[:,i])\n",
    "        pi[i] = nk/w.shape[0]\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e)\n",
    "Define the EM method, and use the log likelihood to check if it is incresing for each iteration of EM or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em(X,k,pi0,mu0,cov0,nIter):\n",
    "    # init for each parameters\n",
    "    pi = pi0\n",
    "    mu = mu0\n",
    "    cov = cov0\n",
    "    # for each iteration\n",
    "    for i in range(nIter):\n",
    "        #First use the E step, to get the gamma\n",
    "        gamma = expection(X,k,pi,mu,cov)\n",
    "        #Then use the M step to maximize pi,mu,cov\n",
    "        pi = MaximizeMixtures(k,gamma)\n",
    "        mu = MaximizeMean(X,k,gamma)\n",
    "        cov = MaximizeCovariance(X,k,gamma,mu0)\n",
    "        print(\"Iteration:\" + \" \", i)\n",
    "        # Use the log likelihood to check if it is incresing or not\n",
    "        result = 0\n",
    "        for n in range(X.shape[0]):\n",
    "            result_sub = 0\n",
    "            for kloop in range(k):\n",
    "                distr = multivariate_normal(mu[kloop],cov[kloop])\n",
    "                result_sub += (pi[kloop]+0.00001)+ distr.pdf(X[n,:])\n",
    "            result += np.log(result_sub)\n",
    "        print(result)\n",
    "    \n",
    "    return mu,cov,pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init parameters, such as X,k,pi0,mu0,cov0,nIter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data\n",
    "k = 5\n",
    "pi0 = np.array([1.0 / 5] * 5)\n",
    "mu0 = np.random.rand(5, X.shape[1])\n",
    "cov0 = np.array([np.eye(X.shape[1])]*5)\n",
    "nIter = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Get the result from EM method, and we can see that the log likelihood is increasing for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "49.387055228482254\n",
      "Iteration:  1\n",
      "50.50509066123417\n",
      "Iteration:  2\n",
      "51.37606837915545\n",
      "Iteration:  3\n",
      "52.326484535834915\n",
      "Iteration:  4\n",
      "53.51415870463857\n",
      "Iteration:  5\n",
      "54.97094471404937\n",
      "Iteration:  6\n",
      "56.78393795178275\n",
      "Iteration:  7\n",
      "59.22828335765747\n",
      "Iteration:  8\n",
      "61.91409567177046\n",
      "Iteration:  9\n",
      "63.771183052353756\n",
      "Iteration:  10\n",
      "64.85683043597305\n",
      "Iteration:  11\n",
      "65.51701144785633\n",
      "Iteration:  12\n",
      "65.9205405479128\n",
      "Iteration:  13\n",
      "66.1945743100717\n",
      "Iteration:  14\n",
      "66.43460208284296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.46174325, -1.51949314],\n",
       "        [ 1.70588951,  1.98436295],\n",
       "        [-1.12202722, -1.12883922],\n",
       "        [-1.13950689, -1.15712251],\n",
       "        [-0.69663996,  0.57971282]]), array([[[ 5.24119189, -3.76934984],\n",
       "         [-3.76934984,  9.60878772]],\n",
       " \n",
       "        [[ 4.12263572,  4.61912725],\n",
       "         [ 4.61912725,  6.58878813]],\n",
       " \n",
       "        [[10.74395456,  9.35783412],\n",
       "         [ 9.35783412,  9.8945444 ]],\n",
       " \n",
       "        [[12.09017953,  8.8373457 ],\n",
       "         [ 8.8373457 ,  7.43375999]],\n",
       " \n",
       "        [[ 9.50510033, -6.03698054],\n",
       "         [-6.03698054,  7.37495646]]]), array([0.14537466, 0.1568149 , 0.10677199, 0.19030021, 0.40073823]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em(X,k,pi0,mu0,cov0,nIter)\n",
    "# expection(X,k,pi0,mu0,cov0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
